{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex set of data are generated.\n",
    "A Bayesian neural network is trained and compared to its determinitstic counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from itertools import combinations\n",
    "import shap\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generated with noise and non-linearity\n",
    "\n",
    "num_features = 10\n",
    "\n",
    "data = generate_data(\n",
    "    num_samples=10000,\n",
    "    num_features=num_features,\n",
    "    interaction_formula=lambda features: to_bool((features[\"x1\"] + features[\"x2\"])**2 + np.sqrt(np.abs(features[\"x3\"])) \n",
    "                                        + features[\"x4\"] + features[\"x5\"] + 0.2*features[\"x6\"]\n",
    "                                        + features[\"x7\"]**2*features[\"x8\"] + 3*features[\"x9\"] + 0.1*features[\"x10\"],\n",
    "                                        threshold=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data.columns:\n",
    "    if x != \"y\":\n",
    "        plt.hist(data[x], alpha=0.4, label=x, density=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(data[\"y\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Deterministic NN\n",
    "\n",
    "num_features = 10\n",
    "learning_rate = 0.001\n",
    "#batch_size = 64 \n",
    "\n",
    "model = SimpleNN(num_features=num_features)\n",
    "pos_weight = None#data.y.mean()/(1-data.y.mean())\n",
    "\n",
    "X, y = data.drop(columns=\"y\").to_numpy(), data[\"y\"].to_numpy()\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train model on all data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "early_stopping(model, X_train_tensor, y_train_tensor, pos_weight=pos_weight, lr=learning_rate)\n",
    "\n",
    "outputs = model(X_test_tensor).round()\n",
    "\n",
    "print(\"label 1 ratio = \", outputs.mean())\n",
    "print(\"false_positive = \",  ((outputs == 1) & (y_test_tensor == 0)).sum().item())\n",
    "print(\"false_negative = \", ((outputs == 0) & (y_test_tensor == 1)).sum().item())\n",
    "print(\"f1 score = \", f1_score(y_true=y_test_tensor.detach().numpy(), y_pred=outputs.detach().numpy()))\n",
    "print(\"accuracy = \", accuracy_score(y_true=y_test_tensor.detach().numpy(), y_pred=outputs.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Bayesian NN from pre-trained DNN\n",
    "\n",
    "const_bnn_prior_parameters = {\n",
    "        \"prior_mu\": 0.0,\n",
    "        \"prior_sigma\": 1.0,\n",
    "        \"posterior_mu_init\": 0.0,\n",
    "        \"posterior_rho_init\": -3.0,\n",
    "        \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "        \"moped_enable\": True,  # True to initialize mu/sigma from the pretrained dnn weights\n",
    "        \"moped_delta\": 0.5,\n",
    "}\n",
    "    \n",
    "dnn_to_bnn(model, const_bnn_prior_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train BNN\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "output = model(X_train_tensor)\n",
    "loss = criterion(output, y_train_tensor)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "early_stopping(model, X_train_tensor, y_train_tensor, pos_weight=pos_weight, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "num_monte_carlo_sample = 100\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_mc = []\n",
    "    for mc_run in range(num_monte_carlo_sample):\n",
    "        probs = model(X_test_tensor)\n",
    "        #probs = nn.functional.softmax(logits, dim=-1) # softmax already as\n",
    "        output_mc.append(probs)\n",
    "    output = torch.stack(output_mc)  \n",
    "    pred_mean = output.mean(dim=0)\n",
    "    pred_std = output.std(dim=0)\n",
    "    y_pred = torch.argmax(pred_mean, axis=-1)\n",
    "    test_acc = (y_pred.data.cpu().numpy() == y_test_tensor.data.cpu().numpy()).mean()\n",
    "\n",
    "# %%\n",
    "plt.hist(pred_std)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
