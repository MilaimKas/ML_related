{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to analyse how feedback effect impact the outcome of a model.\n",
    "A feedback loop is, here, defined as the \"contamination\" of training data with past model prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import shap\n",
    "\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data using sklearn classification utilities\n",
    "\n",
    "num_features = 10\n",
    "\n",
    "X, y = generate_data_sklearn(num_features=num_features)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"y\"] = y\n",
    "data[[f\"feature {i}\" for i in range(X.shape[1])]] = X\n",
    "\n",
    "for col in data.columns:\n",
    "    if col != \"y\":\n",
    "        plt.hist(data[col], label=col, alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X, y)\n",
    "clf_pred = clf.predict(X)\n",
    "\n",
    "print(\"label 1 count = \", clf_pred.sum())\n",
    "print(\"false_positive = \",  ((clf_pred == 1) & (y == 0)).sum().item())\n",
    "print(\"false_negative = \", ((clf_pred == 0) & (y == 1)).sum().item())\n",
    "print(\"f1 score = \", f1_score(y_true=y, y_pred=clf_pred))\n",
    "print(\"accuracy = \", accuracy_score(y_true=y, y_pred=clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "early_stopping(model, X_train_tensor, y_train_tensor, pos_weight=0.9/0.1)\n",
    "\n",
    "outputs = model(X_train_tensor).round()\n",
    "\n",
    "print(\"label 1 count = \", outputs.sum())\n",
    "print(\"false_positive = \",  ((outputs == 1) & (y_train_tensor == 0)).sum().item())\n",
    "print(\"false_negative = \", ((outputs == 0) & (y_train_tensor == 1)).sum().item())\n",
    "print(\"f1 score = \", f1_score(y_true=y_train_tensor.detach().numpy(), y_pred=outputs.detach().numpy()))\n",
    "print(\"accuracy = \", accuracy_score(y_true=y_train_tensor.detach().numpy(), y_pred=outputs.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_loop(model_type, X, y, iterations=20, retrain_epochs=100, ground_truth_prop_start_end=[1., 0.05]):\n",
    "    \n",
    "    # initilize model\n",
    "    if model_type == \"nn\":\n",
    "        model = SimpleNN()\n",
    "    else:\n",
    "        model = LogisticRegression()\n",
    "    \n",
    "    # Define the number of samples per iteration\n",
    "    train_size = X.shape[0] // iterations\n",
    "        \n",
    "    # Lists for tracking accuracy\n",
    "    acc = []\n",
    "    acc_truth = []\n",
    "    \n",
    "    false_positive = []\n",
    "    false_positive_truth = []\n",
    "    \n",
    "    false_negative = []\n",
    "    false_negative_truth = []\n",
    "    \n",
    "    f1 = []\n",
    "    f1_truth = []\n",
    "    \n",
    "    shap_values_over_time = []\n",
    "    \n",
    "    confidence = []\n",
    "    \n",
    "    # Define linear decreasing proportion of ground truth labels\n",
    "    coeff = (ground_truth_prop_start_end[0] - ground_truth_prop_start_end[1]) / (iterations-1)\n",
    "    ground_truth_prop = ground_truth_prop_start_end[0] - coeff * np.arange(iterations)\n",
    "            \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Select the subset of data for the current iteration\n",
    "        X_train = X[train_size * i : train_size * (i + 1)]\n",
    "        y_truth = y[train_size * i : train_size * (i + 1)] \n",
    "        \n",
    "        if not isinstance(model, LogisticRegression):\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_truth = torch.tensor(y_truth, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # get y_pseudo label from previous iteration\n",
    "        if i==0:\n",
    "            if isinstance(model, LogisticRegression):\n",
    "                y_pseudo = y_truth.copy()\n",
    "            else:\n",
    "                y_pseudo = y_truth.clone().detach()\n",
    "        else:\n",
    "            if isinstance(model, LogisticRegression):\n",
    "                y_pseudo = model.predict(X_train)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    y_pseudo = model(X_train).round()   \n",
    "                    \n",
    "            # Retrain the model using the mix of pseudo- and true labels\n",
    "            idx_truth = np.random.choice(np.arange(len(y_truth)), size=int(ground_truth_prop[i] * len(y_truth)), replace=False)\n",
    "            y_pseudo[idx_truth] = y_truth[idx_truth]  # Inject some true labels into pseudo-labels\n",
    "                \n",
    "        # weigth the loss in case of inbalance data\n",
    "        pos_weight = (y_pseudo == 0).sum() / (y_pseudo == 1).sum()    \n",
    "        \n",
    "        if isinstance(model, LogisticRegression):\n",
    "            model.fit(X_train, y_pseudo)\n",
    "            outputs = model.predict_proba(X_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "        \n",
    "            # Accuracy metrics\n",
    "            acc.append(accuracy_score(y_pseudo, y_pred))\n",
    "            acc_truth.append(accuracy_score(y_truth, y_pred))\n",
    "            \n",
    "            f1.append(f1_score(y_pseudo, y_pred))\n",
    "            f1_truth.append(f1_score(y_truth, y_pred))\n",
    "            \n",
    "            # False positives and false negatives\n",
    "            fp_pseudo = ((y_pred == 1) & (y_pseudo == 0)).sum() # Predicted 1, but pseudo-label is 0\n",
    "            fn_pseudo = ((y_pred == 0) & (y_pseudo == 1)).sum() # Predicted 0, but pseudo-label is 1\n",
    "            fp_truth = ((y_pred == 1) & (y_truth == 0)).sum()    # Predicted 1, but true label is 0\n",
    "            fn_truth = ((y_pred == 0) & (y_truth == 1)).sum()    # Predicted 0, but true label is 1\n",
    "            \n",
    "            confidence.append(outputs[:,1])  # Apply sigmoid for confidence interpretation\n",
    "            \n",
    "        else:\n",
    "            # Train model with early stopping\n",
    "            early_stopping(model, X_train, y_pseudo, epochs=retrain_epochs, pos_weight=pos_weight)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_train)  # Model prediction (logits)\n",
    "            y_pred = outputs.round()  # Predicted labels after rounding the logits\n",
    "        \n",
    "            # Accuracy metrics\n",
    "            acc.append(accuracy_score(y_pseudo.numpy(), y_pred.numpy()))\n",
    "            acc_truth.append(accuracy_score(y_truth.numpy(), y_pred.numpy()))\n",
    "            \n",
    "            f1.append(f1_score(y_pseudo, y_pred))\n",
    "            f1_truth.append(f1_score(y_truth, y_pred))\n",
    "            \n",
    "            # False positives and false negatives\n",
    "            fp_pseudo = ((y_pred == 1) & (y_pseudo == 0)).sum().item()  # Predicted 1, but pseudo-label is 0\n",
    "            fn_pseudo = ((y_pred == 0) & (y_pseudo == 1)).sum().item()  # Predicted 0, but pseudo-label is 1\n",
    "            fp_truth = ((y_pred == 1) & (y_truth == 0)).sum().item()    # Predicted 1, but true label is 0\n",
    "            fn_truth = ((y_pred == 0) & (y_truth == 1)).sum().item()    # Predicted 0, but true label is 1\n",
    "            \n",
    "            confidence.append(outputs)  # Apply sigmoid for confidence interpretation\n",
    "        \n",
    "        false_positive.append(fp_pseudo)\n",
    "        false_negative.append(fn_pseudo)\n",
    "        false_positive_truth.append(fp_truth)\n",
    "        false_negative_truth.append(fn_truth)\n",
    "        \n",
    "        # SHAP values\n",
    "        if X_train.shape[0] > 100:  # Avoid index errors when sampling for SHAP\n",
    "            X_background = X_train[torch.randint(X_train.shape[0], [100])]  # Smaller sample for SHAP background\n",
    "        else:\n",
    "            X_background = X_train  # Use full data if too small for 100 samples\n",
    "        \n",
    "        X_shap = X_train[torch.randint(X_train.shape[0], [20])]  # Sample 20 instances for SHAP calculation\n",
    "        \n",
    "        if isinstance(model, LogisticRegression):\n",
    "            explainer = shap.LinearExplainer(model, X_background)\n",
    "        else:\n",
    "            explainer = shap.GradientExplainer(model, X_background)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        shap_values_over_time.append(np.mean(np.abs(shap_values), axis=0))\n",
    "                    \n",
    "    return acc, acc_truth, f1, f1_truth, false_positive, false_positive_truth, false_negative, false_negative_truth, shap_values_over_time, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feedback loop simulation for several class balance\n",
    "\n",
    "weight_list = [0.9, 0.6, 0.5]\n",
    "acc_list = []\n",
    "acc_truth_list = []\n",
    "f1_list = []\n",
    "f1_truth_list = []\n",
    "false_positive_list = []\n",
    "false_negative_list = []\n",
    "false_negative_truth_list = []\n",
    "false_positive_truth_list = []\n",
    "confidence_list = []\n",
    "shap_list = []\n",
    "\n",
    "for w in weight_list:\n",
    "        \n",
    "        X, y = make_data(weigths=[w, 1-w])\n",
    "        acc, acc_truth,f1, f1_truth, false_positive, false_positive_truth, false_negative, false_negative_truth, shap_values_over_time, confidence = \\\n",
    "        feedback_loop(\"linear\", X, y, iterations=50, ground_truth_prop_start_end=[1.0, 0.05])\n",
    "        \n",
    "        acc_list.append(acc)\n",
    "        acc_truth_list.append(acc_truth)\n",
    "        f1_list.append(f1)\n",
    "        f1_truth_list.append(f1_truth)\n",
    "        \n",
    "        false_positive_list.append(false_positive)\n",
    "        false_negative_list.append(false_negative)\n",
    "        false_negative_truth_list.append(false_negative_truth)\n",
    "        false_positive_truth_list.append(false_positive_truth)\n",
    "        \n",
    "        confidence_list.append(confidence)\n",
    "        \n",
    "        shap_list.append(shap_values_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap value\n",
    "color1 = \"#D4CC47\"\n",
    "color2 = \"#7C4D8B\"\n",
    "col_list = plt_help.get_color_gradient(color1, color2, len(weight_list))\n",
    "col_list = [\"red\", \"blue\", \"black\"]\n",
    "\n",
    "# Plot the deviations from the ground truth over iterations\n",
    "for a,at,c,w in zip(acc_list, acc_truth_list, col_list, weight_list):\n",
    "    plt.plot(a, label=str(w), color=c)\n",
    "    plt.plot(at, color=c, linestyle=\"dashed\")\n",
    "    \n",
    "#plt.plot(acc_truth_list, color=\"red\", linestyle=\"dashed\", label=\"accuracy truth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the deviations from the ground truth over iterations\n",
    "for a,at,c,w in zip(f1_list, f1_truth_list, col_list, weight_list):\n",
    "    plt.plot(a, label=str(w), color=c)\n",
    "    plt.plot(at, color=c, linestyle=\"dashed\")\n",
    "    \n",
    "#plt.plot(acc_truth_list, color=\"red\", linestyle=\"dashed\", label=\"accuracy truth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the deviations from the ground truth over iterations\n",
    "for a,at,c,w in zip(false_positive_list, false_positive_truth_list, col_list, weight_list):\n",
    "    plt.plot(a, label=str(w), color=c)\n",
    "    plt.plot(at, color=c, linestyle=\"dashed\")\n",
    "    \n",
    "#plt.plot(acc_truth_list, color=\"red\", linestyle=\"dashed\", label=\"accuracy truth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"False positive count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the deviations from the ground truth over iterations\n",
    "for a,at,c,w in zip(false_negative_list, false_negative_truth_list, col_list, weight_list):\n",
    "    plt.plot(a, label=str(w), color=c)\n",
    "    plt.plot(at, color=c, linestyle=\"dashed\")\n",
    "    \n",
    "#plt.plot(acc_truth_list, color=\"red\", linestyle=\"dashed\", label=\"accuracy truth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"False negative count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap value analysis\n",
    "\n",
    "# confidence\n",
    "color1 = \"#D4CC47\"\n",
    "color2 = \"#7C4D8B\"\n",
    "col_list_tmp = plt_help.get_color_gradient(color1, color2, len(confidence_list[0]))\n",
    "\n",
    "for sh,w in zip(shap_list, weight_list):\n",
    "    for i in range(len(sh)):\n",
    "        plt.plot(range(len(sh[i])), sh[i], color=col_list_tmp[i])\n",
    "    plt.xlabel(\"features\")\n",
    "    plt.ylabel(\"shap value\")\n",
    "    plt.xticks(ticks=range(len(sh[-1])), labels=[f\"feature {i}\" for i in range(len(sh[-1]))], rotation=70)\n",
    "    plt.title(f\"Class inbalance = {w}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence\n",
    "color1 = \"#D4CC47\"\n",
    "color2 = \"#7C4D8B\"\n",
    "col_list_tmp = plt_help.get_color_gradient(color1, color2, len(confidence_list[0]))\n",
    "\n",
    "for conf,w in zip(confidence_list, weight_list):\n",
    "    for i in range(len(conf)):\n",
    "        plt.hist(conf[i], color=col_list_tmp[i], bins=10, alpha=0.4)\n",
    "    plt.title(f\"Class inbalance = {w}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
